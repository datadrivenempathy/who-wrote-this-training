{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import harness_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_factory = harness_util.TemplateHarnessFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "                \"corpusCol\": \"title\",\n",
    "                \"lstmSize\": 32,\n",
    "                \"dropoutRate\": 0,\n",
    "                \"kernelRegPenalty\": 0.01,\n",
    "                \"method\": \"sequence\",\n",
    "                \"numWords\": 3000,\n",
    "                \"sourceCol\": \"source\",\n",
    "                \"sourceIdCol\": \"sourceId\",\n",
    "                \"sourceIdVectorCol\": \"sourceIdVector\",\n",
    "                \"tokenVectorCol\": \"tokenVector\",\n",
    "                \"tokensCol\": \"tokens\",\n",
    "                \"maxSeqLen\": 50\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness = harness_factory.build(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = harness.run('who-wrote-this', 'accept-descr-lstm-manual', config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame = results.get_data_frame()\n",
    "model = results.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(numpy.array(target_frame['tokenVector'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mapping = results.get_source_ids()._NumericalSourceIdSet__mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mapping_invert = {}\n",
    "for source in source_mapping:\n",
    "    source_index = source_mapping[source]\n",
    "    target_frame[source + '_prediction'] = predictions[:,source_index]\n",
    "    source_mapping_invert[source_index] = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame['prediction'] = list(map(lambda x: source_mapping_invert[x], numpy.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frame = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frame['title'] = target_frame['title']\n",
    "output_frame['description'] = target_frame['description']\n",
    "output_frame['actualSource'] = target_frame['source']\n",
    "output_frame['setAssignment'] = target_frame['set_assignment']\n",
    "output_frame['cnnScore'] = target_frame['CNN_prediction']\n",
    "output_frame['foxScore'] = target_frame['Fox_prediction']\n",
    "output_frame['dailyMailScore'] = target_frame['Daily Mail_prediction']\n",
    "output_frame['drudgeReportScore'] = target_frame['Drudge Report_prediction']\n",
    "output_frame['newYorkTimesScore'] = target_frame['New York Times_prediction']\n",
    "output_frame['bbcScore'] = target_frame['BBC_prediction']\n",
    "output_frame['breitbartScore'] = target_frame['Breitbart_prediction']\n",
    "output_frame['wallStreetJournalScore'] = target_frame['Wall Street Journal_prediction']\n",
    "output_frame['voxScore'] = target_frame['Vox_prediction']\n",
    "output_frame['nprScore'] = target_frame['NPR_prediction']\n",
    "output_frame['prediction'] = target_frame['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('./articles.db')\n",
    "output_frame.to_sql('predictions', conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame['numTokens'] = target_frame['tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "word_totals = {}\n",
    "\n",
    "for i, row in target_frame.iterrows():\n",
    "    source = row['source']\n",
    "\n",
    "    if not source in word_counts:\n",
    "        word_counts[source] = {}\n",
    "    \n",
    "    for token in row['tokens']:\n",
    "        if not token in word_counts[source]:\n",
    "            word_counts[source][token] = 0\n",
    "        if not token in word_totals:\n",
    "            word_totals[token] = 0\n",
    "        word_counts[source][token] += 1\n",
    "        word_totals[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in word_counts:\n",
    "    for token in word_counts[source]:\n",
    "        word_counts[source][token] /= word_totals[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = {}\n",
    "\n",
    "for source in word_counts:\n",
    "    max_token = -1\n",
    "    max_count = 0\n",
    "    \n",
    "    for token in word_counts[source]:\n",
    "        candidate_count = word_counts[source][token]\n",
    "        if candidate_count > max_count:\n",
    "            max_count = candidate_count\n",
    "            max_token = token\n",
    "    \n",
    "    max_tokens[source] = {'tokenId': max_token, 'percent': max_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = results.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
